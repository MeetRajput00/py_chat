{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MEET\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel, TFAutoModelForCausalLM, AutoTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"tf_model.h5\";: 100%|██████████| 498M/498M [03:00<00:00, 2.76MB/s] \n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 10.4kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 2.89kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 642/642 [00:00<00:00, 45.9kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 752kB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:09<00:00, 46.4kB/s]\n",
      "Downloading (…)\"tf_model.h5\";: 100%|██████████| 1.42G/1.42G [08:55<00:00, 2.65MB/s]\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 13.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "# Load the pre-trained GPT-2 and DialoGPT models\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "dialoGPT_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "dialoGPT_model = TFAutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to manage the conversation history\n",
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.history = \"\"\n",
    "\n",
    "    def add_to_history(self, input_text):\n",
    "        self.history += input_text + \"\\n\"\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def clear_history(self):\n",
    "        self.history = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate text from the ensemble model\n",
    "def generate_text(conversation, input_text):\n",
    "    # Add the user's input to the conversation history\n",
    "    conversation.add_to_history(input_text)\n",
    "\n",
    "    # Generate text using the GPT-2 model, using the conversation history as input\n",
    "    gpt2_input = conversation.get_history()\n",
    "    gpt2_input_ids = gpt2_tokenizer.encode(gpt2_input, return_tensors='tf')\n",
    "    gpt2_outputs = gpt2_model.generate(\n",
    "        gpt2_input_ids,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    gpt2_generated_text = gpt2_tokenizer.decode(gpt2_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Generate text using the DialoGPT model, using the conversation history as input\n",
    "    dialoGPT_input = conversation.get_history()\n",
    "    dialoGPT_input_ids = dialoGPT_tokenizer.encode(dialoGPT_input, return_tensors='tf')\n",
    "    dialoGPT_outputs = dialoGPT_model.generate(\n",
    "        dialoGPT_input_ids,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=1.0,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    dialoGPT_generated_text = dialoGPT_tokenizer.decode(dialoGPT_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Combine the generated text from the two models\n",
    "    ensemble_text = gpt2_generated_text + dialoGPT_generated_text\n",
    "\n",
    "    # Add the generated text to the conversation history\n",
    "    conversation.add_to_history(ensemble_text)\n",
    "\n",
    "    return ensemble_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MEET\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Define an endpoint to handle incoming text messages\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "def chat():\n",
    "    try:\n",
    "        input_text = request.json[\"input_text\"]\n",
    "        conversation_id = request.json[\"conversation_id\"]\n",
    "\n",
    "        if conversation_id not in conversations:\n",
    "            conversations[conversation_id] = Conversation()\n",
    "\n",
    "        conversation = conversations[conversation_id]\n",
    "        generated_text = generate_text(conversation, input_text)\n",
    "\n",
    "        response = {\"generated_text\": generated_text}\n",
    "\n",
    "        return jsonify(response)\n",
    "    except Exception as e:\n",
    "        # log the error message\n",
    "        print(f\"Error in chatbot2 endpoint: {e}\")\n",
    "        \n",
    "        # return an error response to the client\n",
    "        return jsonify({'error': 'Internal server error'})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    conversations = {}\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m     25\u001b[0m     conversations \u001b[39m=\u001b[39m {}\n\u001b[1;32m---> 26\u001b[0m     app\u001b[39m.\u001b[39;49mrun(debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\MEET\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\flask\\app.py:1191\u001b[0m, in \u001b[0;36mFlask.run\u001b[1;34m(self, host, port, debug, load_dotenv, **options)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mwerkzeug\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mserving\u001b[39;00m \u001b[39mimport\u001b[39;00m run_simple\n\u001b[0;32m   1190\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1191\u001b[0m     run_simple(t\u001b[39m.\u001b[39;49mcast(\u001b[39mstr\u001b[39;49m, host), port, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[0;32m   1192\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1193\u001b[0m     \u001b[39m# reset the first request information if the development server\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m     \u001b[39m# reset normally.  This makes it possible to restart the server\u001b[39;00m\n\u001b[0;32m   1195\u001b[0m     \u001b[39m# without reloader and that stuff from an interactive shell.\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_got_first_request \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MEET\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\werkzeug\\serving.py:1059\u001b[0m, in \u001b[0;36mrun_simple\u001b[1;34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, exclude_patterns, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_reloader\u001b[39;00m \u001b[39mimport\u001b[39;00m run_with_reloader\n\u001b[0;32m   1058\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1059\u001b[0m     run_with_reloader(\n\u001b[0;32m   1060\u001b[0m         srv\u001b[39m.\u001b[39;49mserve_forever,\n\u001b[0;32m   1061\u001b[0m         extra_files\u001b[39m=\u001b[39;49mextra_files,\n\u001b[0;32m   1062\u001b[0m         exclude_patterns\u001b[39m=\u001b[39;49mexclude_patterns,\n\u001b[0;32m   1063\u001b[0m         interval\u001b[39m=\u001b[39;49mreloader_interval,\n\u001b[0;32m   1064\u001b[0m         reloader_type\u001b[39m=\u001b[39;49mreloader_type,\n\u001b[0;32m   1065\u001b[0m     )\n\u001b[0;32m   1066\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1067\u001b[0m     srv\u001b[39m.\u001b[39mserver_close()\n",
      "File \u001b[1;32mc:\\Users\\MEET\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\werkzeug\\_reloader.py:444\u001b[0m, in \u001b[0;36mrun_with_reloader\u001b[1;34m(main_func, extra_files, exclude_patterns, interval, reloader_type)\u001b[0m\n\u001b[0;32m    442\u001b[0m             reloader\u001b[39m.\u001b[39mrun()\n\u001b[0;32m    443\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         sys\u001b[39m.\u001b[39;49mexit(reloader\u001b[39m.\u001b[39;49mrestart_with_reloader())\n\u001b[0;32m    445\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    446\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02eb3cdc95de41ae118e8ca14830085a0dd457628e61fdf59d285012ea7e91b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
